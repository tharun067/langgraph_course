{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxcMOxzHLZSv"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --quiet -U langchain_openai langchain_core langgraph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -qU \"langchain[google-genai]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EX3Cc8PzQFl",
        "outputId": "3ddc2edd-d48e-4a26-91de-c38d1e6bc7b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Messages\n",
        "Chat models can use messages, which capture different roles within a conversation.\n",
        "\n",
        "LangChain supports various message types, including HumanMessage, AIMessage, SystemMessage, and ToolMessage.\n",
        "\n",
        "These represent a message from the user, from chat model, for the chat model to instruct behavior, and from a tool call.\n",
        "\n",
        "Let's create a list of messages.\n",
        "\n",
        "Each message can be supplied with a few things:\n",
        "\n",
        "*   content - content of the message\n",
        "*   name - optionally, a message author\n",
        "*   response_metadata - optionally, a dict of metadata (e.g., often populated by model provider for AIMessages)"
      ],
      "metadata": {
        "id": "IAYiqgKjOJRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "messages = [AIMessage(content=f\"So you said you were researching ocean mammals?\")]\n",
        "messages.append(HumanMessage(content=f\"Yes, that's right.\"))\n",
        "messages.append(AIMessage(content=f\"Great, what would you like to learn about.\"))\n",
        "messages.append(HumanMessage(content=f\"I want to learn about the best place to see Orcas in the US.\"))\n",
        "\n",
        "for m in messages:\n",
        "    m.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xw3Nsu4aODTH",
        "outputId": "0709b8ee-53d5-40f4-8cde-15d40784b2b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "So you said you were researching ocean mammals?\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Yes, that's right.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Great, what would you like to learn about.\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "I want to learn about the best place to see Orcas in the US.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat Models\n",
        "Chat models can use a sequence of message as input and support message types, as discussed above.\n",
        "\n",
        "There are many to choose from! Let's work with GeminiAI."
      ],
      "metadata": {
        "id": "hhDfk85dOteq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, getpass\n",
        "def __set_env(var: str):\n",
        "  if not os.environ.get(var):\n",
        "    os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "__set_env(\"GOOGLE_API_KEY\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYlbQKR7Orvl",
        "outputId": "8280eddb-b2c6-4782-e6bc-a8f1227b14af"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GOOGLE_API_KEY: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "llm = init_chat_model(\"gemini-2.5-flash\",model_provider=\"google_genai\")\n",
        "result = llm.invoke(messages)\n",
        "type(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "yd1nsxTmP68f",
        "outputId": "153a8971-6eaf-4132-a88a-fadefdf9d69e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain_core.messages.ai.AIMessage"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_core.messages.ai.AIMessage</b><br/>def __init__(content: Union[str, list[Union[str, dict]]], **kwargs: Any) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/langchain_core/messages/ai.py</a>Message from an AI.\n",
              "\n",
              "AIMessage is returned from a chat model as a response to a prompt.\n",
              "\n",
              "This message represents the output of the model and consists of both\n",
              "the raw output as returned by the model together standardized fields\n",
              "(e.g., tool calls, usage metadata) added by the LangChain framework.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 154);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "YYsu1bv2QYYa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd32021b-b6f3-406a-c8c5-12a0b0122a6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='That\\'s a fantastic goal! Orcas are incredible creatures.\\n\\nWhen it comes to the \"best\" place to see Orcas in the US, it really depends on what you\\'re looking for, but there are two primary contenders, with a third strong option:\\n\\n1.  **San Juan Islands, Washington (and surrounding Puget Sound):**\\n    *   **Why it\\'s often considered the \"best\":** This region is historically famous for being home to the **Southern Resident Killer Whales (SRKWs)**, a distinct population that primarily eats salmon. The summer months (June to September) were traditionally the most reliable time to see them as they followed the salmon runs.\\n    *   **What to expect now:** While still a prime location, the SRKW population is endangered and their movements have become less predictable in recent years due to declining salmon stocks. However, you are now *very* likely to see **Bigg\\'s Orcas (or Transient Orcas)** in this area year-round. These are mammal-eating orcas that hunt seals, sea lions, and porpoises, and their population is thriving.\\n    *   **Best Time:** June to September for the highest chance of seeing *any* orcas, but Bigg\\'s Orcas can be seen year-round.\\n    *   **Accessibility:** Relatively easy to access from Seattle. Many tour operators depart from Friday Harbor (San Juan Island), Anacortes, or Seattle itself.\\n    *   **Experience:** Beautiful scenery, often combined with sightings of other marine life like humpback whales, minke whales, seals, and porpoises.\\n\\n2.  **Southeast Alaska (e.g., Juneau, Seward, Kenai Fjords, Prince William Sound):**\\n    *   **Why it\\'s great:** Alaska boasts a much larger and healthier population of orcas, including both resident (fish-eating, but different pods than the SRKWs) and transient (mammal-eating) populations. The sheer wilderness and abundance of marine life make for an unforgettable experience.\\n    *   **What to expect:** High chances of seeing orcas, often in spectacular natural settings. Tours often combine orca viewing with glaciers, other whales (humpbacks), sea otters, puffins, and eagles.\\n    *   **Best Time:** Mid-May through September.\\n    *   **Accessibility:** Requires flying to Alaskan cities, then often taking a boat tour.\\n    *   **Experience:** More remote, wilder, and often combines with other incredible Alaskan wildlife and scenery.\\n\\n3.  **Monterey Bay, California:**\\n    *   **Why it\\'s a strong contender:** While not known for resident pods in the same way as the PNW or Alaska, Monterey Bay is an incredibly rich marine ecosystem. It\\'s particularly known for sightings of **Transient Orcas** that come into the bay to hunt the abundant marine mammals (sea lions, seals, and even other whales).\\n    *   **What to expect:** Less predictable than the other two for *guaranteed* orca sightings, but when they do show up, it can be incredibly dramatic, with active hunting behaviors.\\n    *   **Best Time:** Orcas can be seen year-round, but spring and fall can be particularly good as migratory whales (like gray whales) pass through, attracting transients.\\n    *   **Accessibility:** Easily accessible from major California cities. Many tour operators depart from Monterey or Moss Landing.\\n\\n**Summary & Recommendation:**\\n\\n*   **For the most *consistent* chance of seeing orcas (especially Bigg\\'s Orcas) with a well-established whale watching industry and beautiful scenery:** **San Juan Islands, Washington** remains a top choice.\\n*   **For a wilder, more remote experience with a higher overall population of orcas and other incredible wildlife:** **Southeast Alaska** is an excellent option.\\n*   **For a chance to see dramatic transient orca hunting events in a diverse ecosystem:** **Monterey Bay, California** is worth considering, though sightings are less frequent.\\n\\nNo matter where you go, remember that orcas are wild animals, and sightings are never 100% guaranteed. Choose a reputable whale watching company that prioritizes the animals\\' well-being and follows ethical viewing guidelines. Good luck with your research!', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--a11ba1e9-8d3c-4f0a-ae2e-136e7ae2be5b-0', usage_metadata={'input_tokens': 46, 'output_tokens': 1976, 'total_tokens': 2022, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1073}})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.response_metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "org3AsMHz-bO",
        "outputId": "18363ff0-a4a9-4402-fcf9-dd442fb000ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'prompt_feedback': {'block_reason': 0, 'safety_ratings': []},\n",
              " 'finish_reason': 'STOP',\n",
              " 'model_name': 'gemini-2.5-flash',\n",
              " 'safety_ratings': []}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tools\n",
        "Tools are useful whenever you want a model to interact with external systems.\n",
        "\n",
        "External systems (e.g., APIs) often require a particular input schema or payload, rather than natural language.\n",
        "\n",
        "When we bind an API, for example, as a tool we given the model awareness of the required input schema.\n",
        "\n",
        "The model will choose to call a tool based upon the natural language input from the user.\n",
        "\n",
        "And, it will return an output that adheres to the tool's schema.\n",
        "\n",
        "Many LLM providers support tool calling and tool calling interface in LangChain is simple."
      ],
      "metadata": {
        "id": "vYIgoaT80Eo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Multiply function in our tool\n",
        "def multiply(a: int, b: int) -> int:\n",
        "  \"\"\"Multiply a and b.\n",
        "\n",
        "  Args:\n",
        "    a: first int\n",
        "    b: second int\n",
        "\n",
        "  Returns:\n",
        "    product of a and b\n",
        "  \"\"\"\n",
        "  return a*b\n",
        "llm_with_tools = llm.bind_tools([multiply])"
      ],
      "metadata": {
        "id": "yj6qwER60Bof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we pass an input - e.g., \"What is 2 multiplied by 3\" - we see a tool call returned.\n",
        "\n",
        "The tool call has specific arguments that match the input schema of our function along with the name of the function to call.\n",
        "\n",
        "\n",
        "```\n",
        "{'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "vqD94oLT0-Df"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tool_call = llm_with_tools.invoke([HumanMessage(content=\"What is 2 multiplied by 3\")])"
      ],
      "metadata": {
        "id": "LLr2TtsK0n-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tool_call.tool_calls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuqSBOfr1PT_",
        "outputId": "bfca18d4-9c0f-4287-8f99-374f90f644f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'multiply',\n",
              "  'args': {'a': 2.0, 'b': 3.0},\n",
              "  'id': 'e614a620-d6ba-4aae-8256-d40a4a1c6587',\n",
              "  'type': 'tool_call'}]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using messages as state\n",
        "With these foundations in place, we can now use messages in our graph state.\n",
        "\n",
        "Let's define our state, ```MessagesState```, as a ```TypedDict``` with a single key: ```messages```.\n",
        "\n",
        "```messages``` is simply a list of messages, as we defined above (e.g., ```HumanMessage```, etc)."
      ],
      "metadata": {
        "id": "uMgK6Ii41beo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import TypedDict\n",
        "from langchain_core.messages import AnyMessage\n",
        "\n",
        "class MessageState(TypedDict):\n",
        "  messages: list[AnyMessage]\n"
      ],
      "metadata": {
        "id": "6TReZInR1WNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reducers\n",
        "As we discussed, each node will return a new value for our state key ```messages```.\n",
        "\n",
        "But, this new value will override the prior ```messages``` value.\n",
        "\n",
        "As our graph runs, we want to append messages to our ```messages``` state key.\n",
        "\n",
        "We can use reducer functions to address this.\n",
        "\n",
        "Reducers allow us to specify how state updates are performed.\n",
        "\n",
        "If no reducer function is specified, then it is assumed that updates to the key should override it as we saw before.\n",
        "\n",
        "But, to append messages, we can use the pre-built ```add_messages``` reducer.\n",
        "\n",
        "This ensures that any messages are appended to the existing list of messages.\n",
        "\n",
        "We simply need to annotate our ```messages``` key with the ```add_messages``` reducer function as metadata."
      ],
      "metadata": {
        "id": "SbT6MdQy2f_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "class MessageState(TypedDict):\n",
        "  messages: Annotated[list[AnyMessage], add_messages]"
      ],
      "metadata": {
        "id": "s6GHcXM72IWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since having a list of messages in graph state is so common, LangGraph has a pre-built ```MessagesState```!\n",
        "\n",
        "```MessagesState``` is defined:\n",
        "\n",
        "With a pre-build single ```messages``` key\n",
        "This is a list of ``` AnyMessage``` objects\n",
        "It uses the ```add_messages``` reducer\n",
        "We'll usually use ```MessagesState``` because it is less verbose than defining a custom ```TypedDict```, as shown above."
      ],
      "metadata": {
        "id": "xjng7epz4hJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import MessagesState\n",
        "\n",
        "class MessageState(MessagesState):\n",
        "  # Add any keys needed beyond messages, which is prebuilt\n",
        "  pass"
      ],
      "metadata": {
        "id": "s4Rfhknj4NIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To go a bit deeper, we can see how the ```add_messages``` reducer works in isolation."
      ],
      "metadata": {
        "id": "Vkl8Sty85P_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial state\n",
        "initial_messages = [AIMessage(content=\"Hello! How can I assist you?\"),\n",
        "                    HumanMessage(content=\"I'm looking for information on marine biology.\")\n",
        "                   ]\n",
        "# New message to add\n",
        "new_message = AIMessage(content=\"Sure, I can help with that. What specifically are yoy interested in?\")\n",
        "\n",
        "# Test\n",
        "add_messages(initial_messages, new_message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ms0pMGlv5OP4",
        "outputId": "682092a8-50b0-4e57-f4f0-771bb216c221"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[AIMessage(content='Hello! How can I assist you?', additional_kwargs={}, response_metadata={}, id='1a52c90c-afe3-4792-84a0-c3b55517039a'),\n",
              " HumanMessage(content=\"I'm looking for information on marine biology.\", additional_kwargs={}, response_metadata={}, id='f75c51f5-d0e0-451d-b332-1eddd6bfef6c'),\n",
              " AIMessage(content='Sure, I can help with that. What specifically are yoy interested in?', additional_kwargs={}, response_metadata={}, id='4c75b5f4-2a46-4f25-92e3-c03cd898f1b9')]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Our graph\n",
        "Now, lets use ```MessagesState``` with a graph"
      ],
      "metadata": {
        "id": "axE5YqF877au"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "# Node\n",
        "def tool_calling_llm(state:MessageState):\n",
        "  return {\"messages\":[llm_with_tools.invoke(state[\"messages\"])]}\n",
        "\n",
        "# Build graph\n",
        "builder = StateGraph(MessageState)\n",
        "builder.add_node(\"tool_calling_llm\",tool_calling_llm)\n",
        "builder.add_edge(START,\"tool_calling_llm\")\n",
        "builder.add_edge(\"tool_calling_llm\",END)\n",
        "graph = builder.compile()\n",
        "\n",
        "# View\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "652wqoUJ63As",
        "outputId": "1a4969cf-25bd-4ac1-ee73-b3cb903a5f7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we pass in ```Hello!```, the LLM responds without any tool calls."
      ],
      "metadata": {
        "id": "1lDJgVvv9uba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = graph.invoke({\"messages\": [HumanMessage(content=\"Hello!\")]})\n",
        "for m in messages['messages']:\n",
        "  m.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSNNMTaY8XNi",
        "outputId": "b3be2922-3cbc-43ad-a687-12f72732c8e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Hello!\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hello! I'm a calculator bot, I can multiply two numbers for you. How can I help?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = graph.invoke({\"messages\":HumanMessage(content=\"Multiply 2 and 3\")})\n",
        "for m in messages['messages']:\n",
        "  m.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooD_NZ9y-AzW",
        "outputId": "124a417c-b9c8-4ad4-9777-d9ddc1e44890"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Multiply 2 and 3\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  multiply (aee34058-5d58-4f58-89a0-b31a96d7fb18)\n",
            " Call ID: aee34058-5d58-4f58-89a0-b31a96d7fb18\n",
            "  Args:\n",
            "    a: 2.0\n",
            "    b: 3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0oJUSNFX-PG9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}